{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c955a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Can keep any uses Rope scaling internally\n",
    "dtype = None # Auto detects if the dtype is kept none\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models available from unsloth \n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",  # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",  # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",  # Gemma 2.2x faster!\n",
    "]  # More models at https://huggingface.co/unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model , tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6994608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Dataset/Cleaned_Dataset/cleaned_resume_screening.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc00758",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is the provided resume information of the candidate and also the job description for which the candidate \n",
    "applied for along with the information that if the candidate is fit or unfit for the given job description (0 denotes unfit , 1 denotes fit)\n",
    "\n",
    "Your job as a human resource candidate resume and job description matcher to evaluate if the candidate is fit or unfit.\n",
    "Also use your own logical reasoning as well to predict and contribute to the decision \n",
    "### Resume Text :\n",
    "{}\n",
    "\n",
    "### Job description:\n",
    "{}\n",
    "\n",
    "### Decision\n",
    "{}\n",
    "\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_prompt_func(examples):\n",
    "    # Extract lists of column values\n",
    "    Resume_Text = examples['Resume']\n",
    "    Job_Description = examples['Job_Description']\n",
    "    decision = examples['Decision']\n",
    "    \n",
    "    # Prepare list of processed texts\n",
    "    texts = []\n",
    "    for resume, job_desc, desc in zip(Resume_Text, Job_Description, decision):\n",
    "        text = prompt.format(resume, job_desc, desc) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "        \n",
    "    return texts\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"Dataset/Cleaned_Dataset/cleaned_resume_screening.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig , SFTTrainer\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    formatting_func=format_prompt_func,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        warmup_steps=5,\n",
    "        max_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"./Result/outputs\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d48ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save model with float16 precision\n",
    "# model.save_pretrained_merged(\n",
    "#     \"pretrained_model\",  # folder where model will be saved\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf76322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in 8bit precision\n",
    "model.save_pretrained_merged(\n",
    "    \"pretrained_model\",  # folder where model will be saved\n",
    "    tokenizer,\n",
    "    save_method=\"merged_8bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40052fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in quantized 8 bit model\n",
    "\n",
    "model.save_pretrained_gguf(\"pretrained_model\", tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a380285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just saving lora adapter \n",
    "\n",
    "model.save_pretrained(\"pretrained_model\")\n",
    "tokenizer.save_pretrained(\"pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f92f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = FastLanguageModel.from_pretrained(\n",
    "    \"pretrained_model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
