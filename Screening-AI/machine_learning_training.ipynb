{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_df = pd.read_csv(\"Dataset/Cleaned_Dataset/cleaned_resume_screening.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5917239",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['combined_text'] = cleaned_df['Resume'] + ' [SEP] ' + cleaned_df['Job_Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62da6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(y='Decision', data=cleaned_df)\n",
    "plt.title('Decision Class Distribution')\n",
    "plt.xlabel('Decision')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f497340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Tokenizing the combined text\n",
    "X_tokenized = [simple_preprocess(text) for text in cleaned_df['combined_text']]\n",
    "y = cleaned_df['Decision']\n",
    "\n",
    "# Train Word2Vec model with tokenized text\n",
    "word2vec_model = Word2Vec(sentences=X_tokenized, vector_size=1000, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get the average word vector for a document\n",
    "def get_average_word2vec(tokens_list, model, vector_size=1000):\n",
    "    vectors = [model.wv[token] for token in tokens_list if token in model.wv]\n",
    "    if len(vectors) == 0:  # If no words are found in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Convert the entire dataset into word vectors\n",
    "X_word2vec = np.array([get_average_word2vec(tokens, word2vec_model) for tokens in X_tokenized])\n",
    "\n",
    "# Split the data into train and test sets after converting to word vectors\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vector_data = {\n",
    "    \"X_train\" : X_train,\n",
    "    \"X_test\" : X_test,\n",
    "    \"y_train\" : y_train,\n",
    "    \"y_test\" : y_test\n",
    "}\n",
    "\n",
    "with open(\"Dataset/vector_embeddings.pkl\" , \"wb\") as f :\n",
    "    pickle.dump(obj= vector_data , file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"Dataset/vector_embeddings.pkl\" , \"rb\") as f :\n",
    "    vector_embeddings = pickle.load(file=f)\n",
    "\n",
    "X_train = vector_embeddings['X_train']\n",
    "X_test = vector_embeddings['X_test']\n",
    "y_train = vector_embeddings['y_train']\n",
    "y_test = vector_embeddings['y_test']\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d798b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['combined_text'].head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26482cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_classifier = SGDClassifier(\n",
    "    loss='log_loss',  # Logistic regression (log loss)\n",
    "    penalty='l2',  # Regularization\n",
    "    max_iter=1000,  # Max number of iterations\n",
    "    tol=1e-3,  # Stopping criterion\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "sgd_classifier.partial_fit(X_train, y_train, classes=y.unique())  # Initial fit on the training data\n",
    "\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "sgd_classifier.score(X_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],  # Regularization strength\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],  # Regularization type\n",
    "    'max_iter': [1000, 2000],  # Number of iterations\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling'],  # Learning rate schedule\n",
    "    'tol': [1e-4, 1e-3],  # Tolerance for stopping criteria\n",
    "}\n",
    "\n",
    "# Initialize SGDClassifier\n",
    "sgd_classifier = SGDClassifier()\n",
    "\n",
    "# Initialize GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(sgd_classifier, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Reject', 'Hire'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01053265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score , r2_score\n",
    "\n",
    "\n",
    "print(\"Accuracy :-\" , accuracy_score(y_pred , y_test))\n",
    "print(\"R2 score :-\" , r2_score(y_test , y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57199149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# For binary classification\n",
    "y_prob = sgd_classifier.predict_proba(X_test)[:, 1]  # Probability of \"Hire\"\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "model_data = {\n",
    "    'vectorizer': vectorizer,\n",
    "    'classifier': sgd_classifier\n",
    "}\n",
    "\n",
    "with open('resume_screening_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model and vectorizer saved to 'resume_screening_model.pkl'\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. UNPICKLE (LOAD) AND INFERENCE\n",
    "# -------------------------------\n",
    "# Simulate loading in a new session\n",
    "with open('resume_screening_model.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "\n",
    "vectorizer = loaded['vectorizer']\n",
    "classifier = loaded['classifier']\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b415779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hiring_decision(resume_text, job_description):\n",
    "    # Combine\n",
    "    combined_text = resume_text + \" \" + job_description\n",
    "    combined_text = combined_text.strip()\n",
    "\n",
    "    # Vectorize\n",
    "    text_vector = vectorizer.transform([combined_text])\n",
    "\n",
    "    # Predict\n",
    "    prediction = classifier.predict(text_vector)[0]\n",
    "    probability = classifier.predict_proba(text_vector)[0]\n",
    "\n",
    "    decision = \"Hire\" if prediction == 1 else \"Reject\"\n",
    "    prob_hire = probability[1] if prediction == 1 else probability[0]\n",
    "\n",
    "    return {\n",
    "        'Decision': decision,\n",
    "        'Probability (Hire)': round(probability[1], 4),\n",
    "        'Confidence': round(max(probability), 4)\n",
    "    }\n",
    "\n",
    "sample_idx = 0\n",
    "sample_resume = df['Resume'].iloc[X_test.index[sample_idx]]\n",
    "sample_job = df['Job_Description'].iloc[X_test.index[sample_idx]]\n",
    "true_label = y_test.iloc[sample_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INFERENCE EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True Label: {true_label} ({'Hire' if true_label == 1 else 'Reject'})\")\n",
    "print(f\"Resume snippet: {sample_resume[:100]}...\")\n",
    "print(f\"Job Desc snippet: {sample_job[:100]}...\")\n",
    "\n",
    "result = predict_hiring_decision(sample_resume, sample_job)\n",
    "print(\"\\nModel Prediction:\")\n",
    "for k, v in result.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained model from Sentence-Transformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to calculate cosine similarity and classify the resume\n",
    "def classify_resume(resume_text, job_desc_text):\n",
    "    # Get sentence embeddings for both resume and job description\n",
    "    resume_embedding = model.encode(resume_text)\n",
    "    job_desc_embedding = model.encode(job_desc_text)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity([resume_embedding], [job_desc_embedding])[0][0]\n",
    "    \n",
    "    # Print similarity and classify the resume\n",
    "    print(f\"Cosine Similarity: {similarity * 100:.2f}%\")\n",
    "    \n",
    "    # If similarity is greater than 80%, classify as good match\n",
    "    return similarity\n",
    "\n",
    "# Example usage:\n",
    "resume_text_matching = \"\"\"\n",
    "    Experienced software engineer with a passion for building scalable systems and solving complex problems.\n",
    "    Proficient in Python, Java, and cloud technologies like AWS.\n",
    "    Strong background in algorithms, data structures, and software development methodologies.\n",
    "\"\"\"\n",
    "resume_text_slightly_matching = \"\"\"\n",
    "Experienced software engineer with a strong background in designing and developing software solutions. Proficient in programming languages like Python and Java. Skilled in problem-solving, algorithms, and data structures. I have worked on several projects involving web development and mobile app development. Familiar with Git for version control and experienced in working in agile teams. Looking to apply my technical skills in a new role.\n",
    "\"\"\"\n",
    "\n",
    "resume_text_drastically_matching = \"\"\"\n",
    "Highly skilled artist with a passion for digital painting and illustration. Experienced in Adobe Photoshop, Procreate, and other graphic design tools. Strong portfolio showcasing a variety of digital artwork, including character design and concept art. Avid gamer with a deep interest in fantasy art styles and storytelling through visual media. Looking to transition into a full-time role as a concept artist.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "job_desc_text = \"\"\"\n",
    "    We are looking for a software engineer with experience in cloud technologies and strong problem-solving skills.\n",
    "    Proficiency in Python and Java is required, along with knowledge of AWS.\n",
    "\"\"\"\n",
    "\n",
    "# Classify the resume against the job description\n",
    "result_matching = classify_resume(resume_text_matching, job_desc_text)\n",
    "result_slightly_matching = classify_resume(resume_text_slightly_matching, job_desc_text)\n",
    "result_drastically_matching = classify_resume(resume_text_drastically_matching, job_desc_text)\n",
    "\n",
    "print(f\"Similarity for result_matching :-\" , result_matching)\n",
    "print(f\"Similarity for result_slightly_matching :-\" , result_slightly_matching)\n",
    "print(f\"Similarity for result_drastically_matching :-\" , result_drastically_matching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c54e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wcss = []\n",
    "\n",
    "# Try cluster sizes from 1 to 10\n",
    "for idx in range(1, 11):\n",
    "    km = KMeans(n_clusters=idx, random_state=42)\n",
    "    km.fit(X_train)              # No y_train for KMeans\n",
    "    wcss.append(km.inertia_)     # Inertia = WCSS\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.xlabel(\"Number of clusters (K)\")\n",
    "plt.ylabel(\"WCSS (Inertia)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KMeans(n_clusters=2)\n",
    "knn.fit(X_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy :-\" , accuracy_score(y_test , y_pred))\n",
    "print(\"R2 score :-\" , r2_score(y_test , y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
